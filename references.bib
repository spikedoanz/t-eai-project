@article{dettmers2023qlora,
  title={QLoRA: Efficient Finetuning of Quantized LLMs},
  author={Dettmers, Tim and Pagnoni, Artidoro and Holtzman, Ari and Zettlemoyer, Luke},
  journal={arXiv preprint arXiv:2305.14314},
  year={2023}
}

@article{frantar2022gptq,
  title={GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers},
  author={Frantar, Elias and Ashkboos, Saleh and Hoefler, Torsten and Alistarh, Dan},
  journal={arXiv preprint arXiv:2210.17323},
  year={2022}
}

@article{lin2023awq,
  title={AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration},
  author={Lin, Ji and Tang, Jiaming and Tang, Haotian and Yang, Shang and Dang, Xingyu and Han, Song},
  journal={arXiv preprint arXiv:2306.00978},
  year={2023}
}

@misc{llamacpp,
  author={Gerganov, Georgi and contributors},
  title={llama.cpp: Inference of LLaMA model in pure C/C++},
  year={2023},
  publisher={GitHub},
  howpublished={\url{https://github.com/ggerganov/llama.cpp}},
}

@misc{tinygrad,
  author={Hotz, George and contributors},
  title={tinygrad: A simple and powerful deep learning framework},
  year={2023},
  publisher={GitHub},
  howpublished={\url{https://github.com/tinygrad/tinygrad}},
}

@misc{mlcllm,
  author={MLC Team},
  title={MLC-LLM: Machine Learning Compilation for Large Language Models},
  year={2023},
  publisher={GitHub},
  howpublished={\url{https://github.com/mlc-ai/mlc-llm}},
}

@misc{verifiers,
  author={Prime Intellect},
  title={Verifiers: A framework for LLM evaluation},
  year={2024},
  publisher={GitHub},
  howpublished={\url{https://github.com/PrimeIntellect-ai/verifiers}},
}

@article{touvron2023llama,
  title={Llama 2: Open Foundation and Fine-Tuned Chat Models},
  author={Touvron, Hugo and Martin, Louis and Stone, Kevin and others},
  journal={arXiv preprint arXiv:2307.09288},
  year={2023}
}

@article{dubey2024llama3,
  title={The Llama 3 Herd of Models},
  author={Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and others},
  journal={arXiv preprint arXiv:2407.21783},
  year={2024}
}

@misc{qwen2024,
  author={Qwen Team},
  title={Qwen2.5: A Large Language Model Family},
  year={2024},
  howpublished={\url{https://github.com/QwenLM/Qwen2.5}},
}

@article{cobbe2021gsm8k,
  title={Training Verifiers to Solve Math Word Problems},
  author={Cobbe, Karl and Kosaraju, Vineet and Bavarian, Mohammad and others},
  journal={arXiv preprint arXiv:2110.14168},
  year={2021}
}

@article{hendrycks2021math,
  title={Measuring Mathematical Problem Solving With the MATH Dataset},
  author={Hendrycks, Dan and Burns, Collin and Kadavath, Saurav and others},
  journal={NeurIPS Datasets and Benchmarks},
  year={2021}
}

@article{dettmers2022llmint8,
  title={LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale},
  author={Dettmers, Tim and Lewis, Mike and Belkada, Younes and Zettlemoyer, Luke},
  journal={arXiv preprint arXiv:2208.07339},
  year={2022}
}

@article{xiao2023smoothquant,
  title={SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models},
  author={Xiao, Guangxuan and Lin, Ji and Seznec, Mickael and Wu, Hao and Demouth, Julien and Han, Song},
  journal={arXiv preprint arXiv:2211.10438},
  year={2023}
}

@inproceedings{reddi2020mlperf,
  title={MLPerf Inference Benchmark},
  author={Reddi, Vijay Janapa and Cheng, Christine and Kanter, David and others},
  booktitle={2020 ACM/IEEE 47th Annual International Symposium on Computer Architecture (ISCA)},
  pages={446--459},
  year={2020},
  organization={IEEE}
}

@article{vaswani2017attention,
  title={Attention is All You Need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and others},
  journal={Advances in Neural Information Processing Systems},
  volume={30},
  year={2017}
}
