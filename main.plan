> original slides are in docs/
> for original slides

# unsorted

- move .gguf models into a centralized directory ./models/ instead of having multiple copies
  - have a generalized setup process for downloading models, cloning repos, building everything and so on.

# 1. setup
- ssh instructions into a pixel
- llama.cpp setup
- mlc-llm setup
* tinygrad setup
- (can wait until later) automatic setup script from a curl | sh

# 2. sampling pipeline
? what is the information to gather?
  > get this from slides
  * define the schema
  ? what does llama.cpp benchmark gather?
- modify above pipelines to gather said information

# 3. actually sampling

* run sweep for tinygrad
  - on softmacs
  - on pixel 7 pro
  - on pixel 7
- repeat setup steps for A100 and H100 gpu
- collate data into multiple csvs

# 4. llm benchmarking

- expose an openai v1 compatible endpoint for each backend
? what are good benchmarks to use from verifiers?
- gather benchmarking info for those

# 5. analysis of data

? what's the schema for data?
  ```python
  from typing import TypedDict

  class BenchmarkRow(TypedDict):
    step: int
    enqueue_latency_ms: float
    total_latency_ms: float
    tokens_per_sec: float
    memory_throughput_gb_s: float
    param_throughput_gb_s: float
    generated_text: str
    platform: str
    release: str
    device: str
    username: str
    hostname: str
    size: str
    quantize: str
    seed: int
    uuid: str
  ```

# 6. slides

# 7. writeup
